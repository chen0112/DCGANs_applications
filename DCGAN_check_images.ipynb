{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        resized_image = cv2.resize(img, (224,224))\n",
    "        if img is not None:\n",
    "            images.append(resized_image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_images = load_images_from_folder('./train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_images[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(check_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(check_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape,\\\n",
    "Conv2DTranspose, Conv2D, Flatten, Dropout, Embedding, ReLU, InputSpec\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from IPython import display\n",
    "\n",
    "# libraries for SpectralNorm\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.engine import *\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2212\n",
    "batch_size = 8\n",
    "weight_init_std = 0.02\n",
    "weight_init_mean = 0.0\n",
    "leaky_relu_slope = 0.2\n",
    "downsize_factor = 2\n",
    "dropout_rate = 0.3\n",
    "scale_factor = 4 ** downsize_factor\n",
    "lr_decay_steps = 1000\n",
    "noise_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "image_channels = 3\n",
    "image_sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X1.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_initializer = tf.keras.initializers.TruncatedNormal(stddev=weight_init_std, mean=weight_init_mean,\n",
    "                                                          seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposed_conv(model, out_channels, ksize, stride_size, ptype='same'):\n",
    "    model.add(Conv2DTranspose(out_channels, (ksize, ksize),\n",
    "                              strides=(stride_size, stride_size), padding=ptype, \n",
    "                              kernel_initializer=weight_initializer, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(image_width // scale_factor * image_height // scale_factor * 128,\n",
    "                    input_shape=(noise_dim,), kernel_initializer=weight_initializer))\n",
    "    #model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MOMENTUM))\n",
    "    model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
    "    model.add(Reshape((image_height // scale_factor, image_width // scale_factor, 128)))\n",
    "    \n",
    "    model = transposed_conv(model, 512, ksize=5, stride_size=1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model = transposed_conv(model, 256, ksize=5, stride_size=2)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model = transposed_conv(model, 128, ksize=5, stride_size=2)\n",
    "    model = transposed_conv(model, 64, ksize=5, stride_size=2)\n",
    "    model = transposed_conv(model, 32, ksize=5, stride_size=2)\n",
    "    \n",
    "    model.add(Dense(3, activation='tanh', kernel_initializer=weight_initializer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random noise vector\n",
    "noise = tf.random.normal([1,noise_dim])\n",
    "#sample = generate_latent_points(100, 50)\n",
    "# run the generator model with the noise vector as input\n",
    "generated_image = generator(noise, training=False)\n",
    "# display output\n",
    "plt.imshow(generated_image[0, :, :, :])\n",
    "print(generated_image.shape)\n",
    "\n",
    "print(noise.shape, tf.math.reduce_mean(noise).numpy(), tf.math.reduce_std(noise).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseSN(Dense):\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                                 initializer=initializers.RandomNormal(0, 1),\n",
    "                                 name='sn',\n",
    "                                 trainable=False)\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                 W_bar = K.reshape(W_bar, W_shape)  \n",
    "        output = K.dot(inputs, W_bar)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output \n",
    "class ConvSN2D(Conv2D):\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n",
    "                         initializer=initializers.RandomNormal(0, 1),\n",
    "                         name='sn',\n",
    "                         trainable=False)\n",
    "        \n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "    def call(self, inputs, training=None):\n",
    "        def _l2normalize(v, eps=1e-12):\n",
    "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
    "        def power_iteration(W, u):\n",
    "            #Accroding the paper, we only need to do power iteration one time.\n",
    "            _u = u\n",
    "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
    "            _u = _l2normalize(K.dot(_v, W))\n",
    "            return _u, _v\n",
    "        #Spectral Normalization\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        #Flatten the Tensor\n",
    "        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n",
    "        _u, _v = power_iteration(W_reshaped, self.u)\n",
    "        #Calculate Sigma\n",
    "        sigma=K.dot(_v, W_reshaped)\n",
    "        sigma=K.dot(sigma, K.transpose(_u))\n",
    "        #normalize it\n",
    "        W_bar = W_reshaped / sigma\n",
    "        #reshape weight tensor\n",
    "        if training in {0, False}:\n",
    "            W_bar = K.reshape(W_bar, W_shape)\n",
    "        else:\n",
    "            with tf.control_dependencies([self.u.assign(_u)]):\n",
    "                W_bar = K.reshape(W_bar, W_shape)\n",
    "                \n",
    "        outputs = K.conv2d(\n",
    "                inputs,\n",
    "                W_bar,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convSN(model, out_channels, ksize, stride_size):\n",
    "    model.add(ConvSN2D(out_channels, (ksize, ksize), strides=(stride_size, stride_size), padding='same',\n",
    "                     kernel_initializer=weight_initializer, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
    "    #model.add(Dropout(dropout_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(spectral_normalization=True):\n",
    "    model = Sequential()\n",
    "    if spectral_normalization:\n",
    "        model.add(ConvSN2D(64, (5, 5), strides=(1,1), padding='same', use_bias=False,\n",
    "                         input_shape=[image_height, image_width, image_channels], \n",
    "                         kernel_initializer=weight_initializer))\n",
    "        #model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MOMENTUM))\n",
    "        model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model = convSN(model, 64, ksize=5, stride_size=2)\n",
    "        #model = convSN(model, 128, ksize=3, stride_size=1)\n",
    "        model = convSN(model, 128, ksize=5, stride_size=2)\n",
    "        #model = convSN(model, 256, ksize=3, stride_size=1)\n",
    "        model = convSN(model, 256, ksize=5, stride_size=2)\n",
    "        #model = convSN(model, 512, ksize=3, stride_size=1)\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(DenseSN(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2,2), padding='same', use_bias=False,\n",
    "                         input_shape=[image_height, image_width, image_channels], \n",
    "                         kernel_initializer=weight_initializer))\n",
    "        #model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MOMENTUM))\n",
    "        model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model = conv(model, 64, ksize=4, stride_size=2)\n",
    "        #model = convSN(model, 128, ksize=3, stride_size=1)\n",
    "        model = conv(model, 128, ksize=4, stride_size=2)\n",
    "        #model = convSN(model, 256, ksize=3, stride_size=1)\n",
    "        model = conv(model, 256, ksize=4, stride_size=2)\n",
    "        #model = convSN(model, 512, ksize=3, stride_size=1)\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator(spectral_normalization=True)\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_positive_labels(y):\n",
    "    return y - 0.3 + (np.random.random(y.shape) * 0.5)\n",
    "\n",
    "def smooth_negative_labels(y):\n",
    "    return y + np.random.random(y.shape) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly flip some labels\n",
    "def noisy_labels(y, p_flip):\n",
    "    # determine the number of labels to flip\n",
    "    n_select = int(p_flip * int(y.shape[0]))\n",
    "    # choose labels to flip\n",
    "    flip_ix = np.random.choice([i for i in range(int(y.shape[0]))], size=n_select)\n",
    "    \n",
    "    op_list = []\n",
    "    # invert the labels in place\n",
    "    #y_np[flip_ix] = 1 - y_np[flip_ix]\n",
    "    for i in range(int(y.shape[0])):\n",
    "        if i in flip_ix:\n",
    "            op_list.append(tf.subtract(1, y[i]))\n",
    "        else:\n",
    "            op_list.append(y[i])\n",
    "    \n",
    "    outputs = tf.stack(op_list)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.optimizers.Adam(beta_1=0.5)\n",
    "discriminator_optimizer = tf.optimizers.Adam(beta_1=0.5)\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, loss_func, apply_label_smoothing=True, label_noise=True):\n",
    "    if label_noise and apply_label_smoothing:\n",
    "        real_output_noise = noisy_labels(tf.ones_like(real_output), 0.05)\n",
    "        fake_output_noise = noisy_labels(tf.zeros_like(fake_output), 0.05)\n",
    "        real_output_smooth = smooth_positive_labels(real_output_noise)\n",
    "        fake_output_smooth = smooth_negative_labels(fake_output_noise)\n",
    "        if loss_func == 'gan': \n",
    "            real_loss = cross_entropy(tf.ones_like(real_output_smooth), real_output)\n",
    "            fake_loss = cross_entropy(tf.zeros_like(fake_output_smooth), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output_smooth - tf.reduce_mean(fake_output_smooth) - tf.ones_like(real_output_smooth)))\n",
    "        + tf.reduce_mean(tf.square(fake_output_smooth - tf.reduce_mean(real_output_smooth) + tf.ones_like(fake_output_smooth)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output_smooth)\n",
    "                avg_real_logit = tf.reduce_mean(real_output_smooth)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output_smooth - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output_smooth - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_r_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_f_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output_smooth) - (real_output_smooth - tf.reduce_mean(fake_output_smooth))))\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output_smooth) + (fake_output_smooth - tf.reduce_mean(real_output_smooth))))\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    elif label_noise and not apply_label_smoothing:\n",
    "        real_output_noise = noisy_labels(tf.ones_like(real_output), 0.05)\n",
    "        fake_output_noise = noisy_labels(tf.zeros_like(fake_output), 0.05)\n",
    "        if loss_func == 'gan': \n",
    "            real_loss = cross_entropy(tf.ones_like(real_output_noise), real_output)\n",
    "            fake_loss = cross_entropy(tf.zeros_like(fake_output_noise), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output_noise - tf.reduce_mean(fake_output_noise) - tf.ones_like(real_output_noise)))\n",
    "        + tf.reduce_mean(tf.square(fake_output_noise - tf.reduce_mean(real_output_noise) + tf.ones_like(fake_output_noise)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output_noise)\n",
    "                avg_real_logit = tf.reduce_mean(real_output_noise)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output_noise - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output_noise - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_r_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_f_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output_noise) - (real_output_noise - tf.reduce_mean(fake_output_noise))))\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output_noise) + (fake_output_noise - tf.reduce_mean(real_output_noise))))\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    elif apply_label_smoothing and not label_noise:\n",
    "        real_output_smooth = smooth_positive_labels(tf.ones_like(real_output))\n",
    "        fake_output_smooth = smooth_negative_labels(tf.zeros_like(fake_output))\n",
    "        if loss_func == 'gan': \n",
    "            real_loss = cross_entropy(tf.ones_like(real_output_smooth), real_output)\n",
    "            fake_loss = cross_entropy(tf.zeros_like(fake_output_smooth), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output_smooth - tf.reduce_mean(fake_output_smooth) - tf.ones_like(real_output_smooth)))\n",
    "        + tf.reduce_mean(tf.square(fake_output_smooth - tf.reduce_mean(real_output_smooth) + tf.ones_like(fake_output_smooth)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output_smooth)\n",
    "                avg_real_logit = tf.reduce_mean(real_output_smooth)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output_smooth - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output_smooth - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_r_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_f_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output_smooth) - (real_output_smooth - tf.reduce_mean(fake_output_smooth))))\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output_smooth) + (fake_output_smooth - tf.reduce_mean(real_output_smooth))))\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss    \n",
    "    else:\n",
    "        if loss_func == 'gan': \n",
    "            real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "            fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output - tf.reduce_mean(fake_output) - tf.ones_like(real_output)))\n",
    "        + tf.reduce_mean(tf.square(fake_output - tf.reduce_mean(real_output) + tf.ones_like(fake_output)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output)\n",
    "                avg_real_logit = tf.reduce_mean(real_output)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_r_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_f_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output) - (real_output - tf.reduce_mean(fake_output))))\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output) + (fake_output - tf.reduce_mean(real_output))))\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, loss_func, apply_label_smoothing=True):\n",
    "    if apply_label_smoothing:\n",
    "        fake_output_smooth = smooth_negative_labels(tf.ones_like(fake_output))\n",
    "        if loss_func == 'gan':\n",
    "            return cross_entropy(tf.ones_like(fake_output_smooth), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output - tf.reduce_mean(fake_output_smooth) + tf.ones_like(real_output)))\n",
    "        + tf.reduce_mean(tf.square(fake_output_smooth - tf.reduce_mean(real_output) - tf.ones_like(fake_output_smooth)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output_smooth)\n",
    "                avg_real_logit = tf.reduce_mean(real_output)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output_smooth - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_f_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_r_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output_smooth) - (fake_output_smooth - tf.reduce_mean(real_output))))\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output) + (real_output - tf.reduce_mean(fake_output_smooth))))\n",
    "                loss = fake_loss + real_loss\n",
    "                return loss\n",
    "    else:           \n",
    "        if loss_func == 'gan':\n",
    "            return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        else:\n",
    "            if loss_func == 'ralsgan':\n",
    "                return (tf.reduce_mean(tf.square(real_output - tf.reduce_mean(fake_output) + tf.ones_like(real_output)))\n",
    "        + tf.reduce_mean(tf.square(fake_output - tf.reduce_mean(real_output) - tf.ones_like(fake_output)))) / 2.\n",
    "            elif loss_func == 'rasgan':\n",
    "                avg_fake_logit = tf.reduce_mean(fake_output)\n",
    "                avg_real_logit = tf.reduce_mean(real_output)\n",
    "                D_r_tilde = tf.nn.sigmoid(real_output - avg_fake_logit)\n",
    "                D_f_tilde = tf.nn.sigmoid(fake_output - avg_real_logit)\n",
    "                total_loss = - tf.reduce_mean(tf.math.log(\n",
    "                    D_f_tilde + 1e-14)) - tf.reduce_mean(tf.math.log(1 - D_r_tilde + 1e-14))\n",
    "                return total_loss\n",
    "            elif loss_func == 'rahinge':\n",
    "                fake_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(fake_output) - (fake_output - tf.reduce_mean(real_output))))\n",
    "                real_loss = tf.reduce_mean(tf.nn.relu(tf.ones_like(real_output) + (real_output - tf.reduce_mean(fake_output))))\n",
    "                loss = fake_loss + real_loss\n",
    "                return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "num_examples_to_generate = 64\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, loss_type='gan'):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(real_output, fake_output, loss_type, apply_label_smoothing=True)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, loss_type, \n",
    "                                       apply_label_smoothing=True, label_noise=True)\n",
    " \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(G_losses, D_losses, all_gl, all_dl, epoch):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss - EPOCH {}\".format(epoch))\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    ymax = plt.ylim()[1]\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(len(all_gl)),all_gl,label='G')\n",
    "    plt.plot(np.arange(len(all_dl)),all_dl,label='D')\n",
    "    plt.legend()\n",
    "    #plt.ylim((0,np.min([1.1*np.max(all_gl),2*ymax])))\n",
    "    plt.title('All Time Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, rows, cols):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_image(model, noise_dim=noise_dim):\n",
    "    test_input = tf.random.normal([1, noise_dim])\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.imshow((predictions[0, :, :, :] * 127.5 + 127.5) / 255.)\n",
    "    plt.axis('off') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_initial_d = 0.0001\n",
    "lr_initial_g = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path('./output_images/')\n",
    "def save_images(directory=OUT_DIR):\n",
    "    for k in range(image_sample_size):\n",
    "        generated_image = generator(tf.random.normal([1, noise_dim]), training=False)\n",
    "        f = str(k)+'.png'\n",
    "        f = os.path.join(directory, f)\n",
    "        img = np.array(generated_image)\n",
    "        img = (img[0, :, :, :] + 1.) / 2.\n",
    "        #img = np.squeeze(img, axis=2)  # axis=2 is channel dimension \n",
    "        img = Image.fromarray((255*img).astype(np.uint8))\n",
    "        img.save(f,'PNG')\n",
    "        #if k % 1000==0: print(k)\n",
    "    print('Saved temporary images for evaluation.')\n",
    "def zip_images(filename='images.zip'):\n",
    "    # SAVE TO ZIP FILE NAMED IMAGES.ZIP\n",
    "    z = zipfile.PyZipFile(filename, mode='w')\n",
    "    for k in range(image_sample_size):\n",
    "        generated_image = generator(tf.random.normal([1, noise_dim]), training=False)\n",
    "        f = str(k)+'.png'\n",
    "        img = np.array(generated_image)\n",
    "        img = (img[0, :, :, :] + 1.) / 2.\n",
    "        img = np.squeeze(img, axis=2)\n",
    "        img = Image.fromarray((255*img).astype(np.uint8))\n",
    "        img.save(f,'PNG')\n",
    "        z.write(f)\n",
    "        os.remove(f)\n",
    "        #if k % 1000==0: print(k)\n",
    "    z.close()\n",
    "    print('Saved final images for submission.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results = 30\n",
    "\n",
    "decay_step = 50\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    all_gl = np.array([]); all_dl = np.array([])\n",
    "    \n",
    "    exp_replay = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        G_loss = []; D_loss = []\n",
    "        \n",
    "        start = time.time()\n",
    "        new_lr_d = lr_initial_d\n",
    "        new_lr_g = lr_initial_g\n",
    "        global_step = 0\n",
    "        \n",
    "        for image_batch in dataset:\n",
    "            g_loss, d_loss = train_step(image_batch)\n",
    "            global_step = global_step + 1\n",
    "            G_loss.append(g_loss); D_loss.append(d_loss)\n",
    "            all_gl = np.append(all_gl,np.array([G_loss]))\n",
    "            all_dl = np.append(all_dl,np.array([D_loss]))\n",
    "\n",
    "         \n",
    "        #display.clear_output(wait=True)\n",
    "        if (epoch + 1) % display_results == 0 or epoch == 0:\n",
    "            plot_losses(G_loss, D_loss, all_gl, all_dl, epoch + 1)\n",
    "            generate_and_save_images(generator, epoch + 1, seed, rows=8, cols=8)\n",
    "        \n",
    "                \n",
    "        # expotentialdecay learning rate decay\n",
    "        if (epoch + 1) % decay_step == 0:\n",
    "            new_lr_d = lr_schedule\n",
    "            new_lr_g = lr_schedule\n",
    "            generator_optimizer = tf.optimizers.Adam(learning_rate=new_lr_d, beta_1=0.5)\n",
    "            discriminator_optimizer = tf.optimizers.Adam(learning_rate=new_lr_g, beta_1=0.5)          \n",
    "\n",
    "        print('Epoch: {} computed for {} sec'.format(epoch + 1, time.time() - start))\n",
    "        print('Gen_loss mean: ', np.mean(G_loss),' std: ', np.std(G_loss))\n",
    "        print('Disc_loss mean: ', np.mean(D_loss),' std: ', np.std(D_loss))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    \n",
    "    generate_and_save_images(generator, epochs, seed, rows=8, cols=8)\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    OUT_DIR.mkdir(exist_ok=True)\n",
    "    save_images(OUT_DIR)\n",
    "      \n",
    "    print('Final epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(X1, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_test_image(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 1*(i**2)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "    IPython.display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
